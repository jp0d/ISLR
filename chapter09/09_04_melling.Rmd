---
title: "ISLR Q9.4"
output:
  html_document:
    toc: true
    depth: 2               
date: "`r format(Sys.time(), '%B %d, %Y')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

p369

Generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation between the two classes. 

 - Show that in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the training data. 
 - Which technique performs best on the test data? 
 - Make plots and report training and test error rates in order to back up your assertions

## 

```{r include=FALSE}
library(tidyverse)
```

2 columns and the class
```{r}
set.seed(1)
x = matrix(rnorm(50*2), ncol=2)
y = c(rep(-1,25), rep(1,25)) # The class vector
x[y==1,]=x[y==1,] + 1
```

```{r}
rnorm(2)
```

```{r}
library(e1071)
dat = data.frame(x=x, y=as.factor(y))
train = sample(50, 25)
```

```{r}
x
```

# Scatter Plot
```{r}
df = data.frame(x1 = x[1], x2=x[2], y=y)
ggplot(df, aes(x=x1, x2, color=factor(y))) +
  geom_point(size=2, shape=23) + 
  xlim(-2, 2) + 
  ylim(-2, 2) + geom_jitter()
```

```{r}
dat
```


### Linear, Cost=1
```{r}
svmfit.linear=svm(y~., data=dat[train,], kernel="linear", cost=1, scale = FALSE)
plot(svmfit.linear, dat)
```

```{r}
summary(svmfit.linear)
```

### Confusion Matrix
```{r}
svm.predict = predict(svmfit.linear, newdata = dat[train,])
table(true=dat[train,"y"], pred=svm.predict)
```
```{r}
(11+7)/25
```
```{r}
length(svm.predict)
```


### Radial, gamma=1, cost=1
```{r}
svmfit.radial=svm(y~., data=dat[train,], kernel="radial", gamma=1,
           cost=1)
plot(svmfit.radial, dat[train,])
```

```{r}
summary(svmfit.radial)
```
```{r}
svm.predict.radial = predict(svmfit.radial, newdata = dat[train,])
table(true=dat[train,"y"], pred=svm.predict.radial)
```

```{r}
(13+9)/25
```
## Do Predict for Test Data

### Linear
```{r}
svm.predict.test = predict(svmfit.linear, newdata = dat[-train,])
table(true=dat[-train,"y"], pred=svm.predict.test)
```

```{r}
(10+11)/25
```
```{r}
plot(svmfit.linear, dat[-train,])
```

### Radial Kernel

```{r}
svm.predict.radial.test = predict(svmfit.radial, newdata = dat[-train,])
table(true=dat[-train,"y"], pred=svm.predict.radial.test)
```

```{r}
(8+9)/25
```

```{r}
plot(svmfit.radial, dat[-train,])
```

The radial kernel overfit the training data and did not do as well on the test data.
In other words, linear kernel was better on the test data.
