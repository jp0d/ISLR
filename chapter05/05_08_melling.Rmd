---
title: "ISLR Q5.8 - Cross-Validation on Simulated Data"
output:
  html_document:
    toc: true
    depth: 2      
---
[ISLR Home](../index.html)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question

p200

 We will now perform cross-validation on a simulated data set.
(a) Generate a simulated data set as follows:
```
> set.seed(1)
> x=rnorm(100)
> y=x-2*x^2+rnorm(100)
```

In this data set, what is n and what is p? Write out the model used to generate the data in equation form.

(b) Create a scatterplot of X against Y . Comment on what you find.

(c) Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:
         
i. Y = β0 + β1X + ε

$Y=\beta_0 + \beta_1 X + \epsilon$

ii. Y = β0 + β1X + β2X2 + ε

$Y=\beta_0 + \beta_1 X + \beta_2 X2 + \epsilon$

iii. Y = β0 +β1X +β2X2 +β3X3 +ε

$Y=\beta_0 + \beta_1 X + \beta_2 X2 + \beta_3 X3 + \epsilon$

iv. Y = β0 +β1X +β2X2 +β3X3 +β4X4 +ε.

$Y=\beta_0 + \beta_1 X + \beta_2 X2 + \beta_3 X3 + \beta_4 X4 + \epsilon$

Note you may find it helpful to use the data.frame() function
to create a single data set containing both X and Y .

(d) Repeat (c) using another random seed, and report your results.
Are your results the same as what you got in (c)? Why?

(e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.

(f) Comment on the statistical significance of the coefficient esti- mates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?

***

```{r message=FALSE, warning=FALSE}
library(ISLR)
```

# 8a

```{r}
set.seed(1)
x=rnorm(100)
y=x - 2*x^2 + rnorm(100)
# n = 100
# p = 2

# Y = b0 + b1x1 + ...+ bpXp 
```


# 8b Plot

```{r}
plot(x,y)
```



# Non-linear relationship.  Quadratic

# 8c

```{r}
library(boot) # cv.glm()

# More on poly()
# https://stackoverflow.com/questions/19484053/what-does-the-r-function-poly-really-do




loocv.fn = function(df, exponent) {
  glm.fit = glm(y~poly(x, exponent), df)
}


set.seed(1)
df = data.frame(x,y)

cv.error=rep(0,5) # 5 zeros
#(0,0,0,0,0)

for (i in 1:5) {
  print(i)
  glm.fit=glm(y~poly(x, i))
  print(cv.glm(df, glm.fit)$delta)
#  cv.error[i] = cv.glm(df, glm.fit)$delta[]
}

#cv.error

```



# 8d

```{r}
set.seed(10)
cv.error=rep(0,5) # 5 zeros

for (i in 1:5) {
  print(i)
  glm.fit=glm(y~poly(x, i))
  #print(cv.glm(df, glm.fit)$delta)
  cv.error[i] = cv.glm(df, glm.fit)$delta[1]
}

plot(1:5, cv.error)

```


We get the same answer because LOOCV only leaves out one data point out
Cycles through all points one at a time to train.

# 8e

Because original equation was quadratic, the quadratic fit will match the best

# 8f

```{r}
summary(glm.fit)
```

The linear and quadratic have significance p-value < 0.05
