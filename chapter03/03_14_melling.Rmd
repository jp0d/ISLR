---
title: "ISLR Q3.14 - Collinearity Problem"
output:
  html_document:
    toc: true
    depth: 2    
---

[ISLR Home](../index.html)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question

This problem focuses on the *collinearity* problem

(a) Perform the following commands in R:
```
> set.seed(1)
> x1=runif(100)
> x2=0.5*x1+rnorm(100)/10
> y=2+2*x1+0.3*x2+rnorm(100)
```

The last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coefficients?

(b) What is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.

(c) Using this data, fit a least squares regression to predict y using x1 and x2. Describe the results obtained. What are $\hat{\beta_0}, \hat{\beta_1}$, and $\hat{\beta_2}$? How do these relate to the true $\beta_0, \beta_1$, and $\beta_2$? Can you reject the null hypothesis $H_0: \beta_1 = 0$? How about the null hypothesis $H_0 : \beta_2 = 0$?

(d) Now fit a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis $H_0: \beta_1 = 0$?

(e) Now fit a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis $H_0: \beta_1 = 0$?

(f) Do the results obtained in (c)â€“(e) contradict each other? Explain your answer.
(g) Now suppose we obtain one additional observation, which was unfortunately mismeasured.

```
> x1=c(x1, 0.1) 
> x2=c(x2, 0.8) 
> y=c(y,6)
```
Re-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.

***

# 14a

```{r}
set.seed(0) # Setting the random seed
```

Generating x1 data using runif (provides uniform distribution from 0-1)
```{r}
x1 = runif(100)
```

Generating data for x2 using random values from normal distribution
```{r}
x2 = 0.5*x1 + rnorm(100)/10
```

Creating a linear model. y is a function of x1 and x2
```{r}
y = 2 + 2*x1 + 0.3*x2 + rnorm(100)
```

- $\beta_0 = -2$
- $\beta_1 = 2$
- $\beta_2 = 0.3$

# 14b Plotting x1 vs x2
```{r}
plot(x1, x2)
```

Based on the graph, there is a postive correlation between x1 and x2

## Correlation between x1 and x2
```{r}
cor(x1, x2) # 0.8324
```

# 14c Fit Model

Fitting the a Least Squares Regression to predict y given x1 and x2
```{r}
lm.fit = lm(y~x1+x2)
```

## Model Summary
```{r}
summary(lm.fit)
```

- $\hat{\beta_0} = 2.0966$
- $\hat{\beta_1} = 2.5370$
- $\hat{\beta_2} = -0.8687$

The coefficient estimates for $\hat{\beta_0}$ and $\hat{\beta_1}$ are similar to true coefficients ($\beta_0$ and $\beta_1$). It is off by at most 0.5 units. 

However the coefficient estimate for Bhat2 is off by 1.1 units. Its extremely high p-value (0.4671) tells us that we cannot reject the null hypothesis, which means that there is a high chance that there is no relationship between y and x2.

## VIF to check for multicollinearity 
```{r}
library(car)
vif(lm.fit)
```

# 14d Creating a new model predicting y using only x1
```{r}
lm.x1 = lm(y~x1)
```

## Model Summary
```{r}
summary(lm.x1)
```

We can reject the null hypothesis because the p-value (4.37E-7) is extremely below 0.05.

# 14e Creating a new model predicting y using only x2
```{r}
lm.x2 = lm(y~x2)
```


## Model Summary

```{r}
summary(lm.x2)
```

We can reject the null hypothesis because the p-value (0.0002) is extremely below 0.05.

# 14f

In (c), the p-value for Bhat2 was not siginificant but in (e) if we do a LSR model separately, then it does become significant. Also the coefficient for Bhat2 changed by almost 2 units from (c) and (e).

# 14g

Adding mismeasured observations into the data

```{r}
x1 = c(x1, 0.1)
x2 = c(x2, 0.8)
y = c(y, 6)
```


## Refitting the data with new data

```{r}
lm.new.fit = lm(y~x1+x2)
summary(lm.new.fit)
```

## Creating a new model predicting y using only x1

```{r}
lm.new.x1 = lm(y~x1)
```

## Model Summary

```{r}
summary(lm.new.x1)
```

## Creating a new model predicting y using only x1

```{r}
lm.new.x2 = lm(y~x2)
```

## Model Summary

```{r}
summary(lm.new.x2)
```

Given the new data, it makes the $R^2$ worse and also the coefficient estimate for Bhat1 is farther that before. 

## Checking for leverage

```{r}
plot(hatvalues(lm.new.fit))
```

Based on the graph above, the new data point has high leverage

## Diagnostic Plots
```{r}
par(mfrow = c(2,2)) # 4 plots in same picture
plot(lm.new.fit)
```

Since the studentized residuals for new data is not greater than 3, we can say it is not an outlier.

