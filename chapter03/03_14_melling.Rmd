---
title: "ISLR Q3.14"
output:
  html_document: default
---

[ISLR Home](../index.html)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
#library(ISLR)
```

# This problem focuses on the collinearity problem

## 14a

```{r}
set.seed(0) # Setting the random seed
```

Generating x1 data using runif (provides uniform distribution from 0-1)
```{r}
x1 = runif(100)
```

Generating data for x2 using random values from normal distribution
```{r}
x2 = 0.5*x1 + rnorm(100)/10
```

Creating a linear model. y is a function of x1 and x2
```{r}
y = 2 + 2*x1 + 0.3*x2 + rnorm(100)
```

- B0 = 2
- B1 = 2
- B2 = 0.3

## 14b

### Plotting x1 vs x2
```{r}
plot(x1, x2)
```

## Based on the graph, there is a postive correlation between x1 and x2

Calculating the correlation between x1 and x2
```{r}
cor(x1, x2) # 0.8324
```

## 14c

Fitting the a Least Squares Regression to predict y given x1 and x2
```{r}
lm.fit = lm(y~x1+x2)
```

### Viewing the statistics of model
```{r}
summary(lm.fit)
```

- Bhat0 = 2.0966
- Bhat1 = 2.5370
- Bhat2 = -0.8687

The coefficient estimates for Bhat0 and Bhat1 are similar to true coefficients (B0 and B1). It is off by at most 0.5 units. 

However the coefficient estimate for Bhat2 is off by 1.1 units. Its extremely high p-value (0.4671) tells us that we cannot reject the null hypothesis, which means that there is a high chance that there is no relationship between y and x2.

### Use VIF to check for multicollinearity 
```{r}
library(car)
vif(lm.fit)
```


## 14d

Creating a new model predicting y using only x1
```{r}
lm.x1 = lm(y~x1)
```


Viewing the statistics of the model
```{r}
summary(lm.x1)
```

We can reject the null hypothesis because the p-value (4.37E-7) is extremely below 0.05.

## 14e

Creating a new model predicting y using only x2
```{r}
lm.x2 = lm(y~x2)
```


Summary statistics of the model

```{r}
summary(lm.x2)
```

We can reject the null hypothesis because the p-value (0.0002) is extremely below 0.05.

## 14f

In (c), the p-value for Bhat2 was not siginificant but in (e) if we do a LSR model 
separately, then it does become significant. Also the coefficient for Bhat2 changed by almost 2 units from (c) and (e).

## 14g

Adding mismeasured observations into the data

```{r}
x1 = c(x1, 0.1)
x2 = c(x2, 0.8)
y = c(y, 6)
```


Refitting the data with new data

```{r}
lm.new.fit = lm(y~x1+x2)
summary(lm.new.fit)
```

### Creating a new model predicting y using only x1

```{r}
lm.new.x1 = lm(y~x1)
```

Statistics of the model

```{r}
summary(lm.new.x1)
```

### Creating a new model predicting y using only x1

```{r}
lm.new.x2 = lm(y~x2)
```

Statistics of the model

```{r}
summary(lm.new.x2)
```

Given the new data, it makes the $$R^2$$ worse and also the coefficient estimate for Bhat1 is farther that before. 

### Checking for leverage
```{r}
plot(hatvalues(lm.new.fit))
```


Based on the graph above, the new data point has high leverage

### Diagnostic Plots
```{r}
par(mfrow = c(2,2)) # 4 plots in same picture
plot(lm.new.fit)
```

Since the studentized residuals for new data is not greater than 3, we can say it is not an outlier.

