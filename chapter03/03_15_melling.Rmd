---
title: "ISLR Q3.15"
output:
  html_document:
    toc: true
    toc_depth: 3
---
[ISLR Home](../index.html)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.

# Notes

I summarize some of what you need to know to better understand linear regression.

## Degrees of freedom
## Residual Standard Error - RSE
## Multiple and Adjusted R-squared
## F-statistic and its p-value

## Four Diagnostic Graphs

- 1
- 2
- Q-Q Quartile/Quartile
- Residuals vs. Leverage graph (bottom right)


```{r message=FALSE, warning=FALSE}
library(MASS)
library(tidyverse)
```

Information on the Boston Housing data can be found [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html)

```{r}
attach(Boston) # Attaching the Boston dataset to workspace

lm.function = function(predictor) {
  
  fit1 <- lm(crim ~ predictor, data = Boston)
  #fit1$coefficients
  # names(fit1$coefficients) <- c('Intercept', predictor)
  return(summary(fit1))
}

# for (v in c(rm, age)) {
#   summary(lm(crim ~ v, data = Boston))
# }
# lm.function(rm)
```

# 15a Simple Linear Regression

(a) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.

***

Fit each feature one at a time and evaluate


## Model: crim ~ zn: $crim = \beta_0 + \beta_1 zn$

$crim = \beta_0 + \beta_1 zn$

## Model Summary
```{r}
lm.zn = lm(crim ~ zn, data = Boston)
summary(lm.zn)
```

Based on the p-value (5.51e-6), zn has a significant association with crim

## Diagnostic Plots

```{r}
par(mfrow = c(2,2))
plot(lm.zn)
```

## Model: crim ~ indus

$crim = \beta_0 + \beta_1 indus$

## Model Summary
```{r}
lm.indus = lm(crim ~ indus, data = Boston)
summary(lm.indus)
```

## Diagnostic Plot
```{r}
par(mfrow = c(2,2))
plot(lm.indus)
```

Based on the p-value (2e-16), indus has a significant association with crim

## Model: crim ~ chas

$crim = \beta_0 + \beta_1 chas$

## Model Summary
```{r}
lm.chas = lm(crim ~ chas, data = Boston)
summary(lm.chas)
```

Based on the p-value (.209), chas does not have an association with crim


## Model: crim ~ nox

$crim = \beta_0 + \beta_1 nox$

## Model Summary
```{r}
lm.nox = lm(crim ~ nox, data = Boston)
summary(lm.nox)
```


Based on the p-value (2e-16), nox has a significant association with crim

## Model: crim ~ rm

$crim = \beta_0 + \beta_1 rm$

## Model Summary
```{r}
lm.rm = lm(crim ~ rm, data = Boston)
summary(lm.rm)
```


Based on the p-value (6.35e-7), rn has a significant association with crim

## Model: crim ~ age

$crim = \beta_0 + \beta_1 age$

## Model Summary
```{r}
lm.age = lm(crim ~ age, data = Boston)
summary(lm.age)
```

Based on the p-value (2.85e-16), age has a significant association with crim

## Model: crim ~ dis

$crim = \beta_0 + \beta_1 dis$

## Model Summary
```{r}
lm.dis = lm(crim ~ dis, data = Boston)
summary(lm.dis)
```

Based on the p-value (2e-16), dis has a significant association with crim

## Model: crim ~ rad

$crim = \beta_0 + \beta_1 rad$
## Model Summary
```{r}
lm.rad = lm(crim ~ rad, data = Boston)
summary(lm.rad)
```

Based on the p-value (2e-16), rad has a significant association with crim

## Model: crim ~ tax

$crim = \beta_0 + \beta_1 tax$

```{r}
lm.tax = lm(crim ~ tax, data = Boston)
summary(lm.tax)
```

Based on the p-value (2e-16), tax has a significant association with crim

## Model: crim ~ ptratio

$crim = \beta_0 + \beta_1 ptratio$

```{r}
lm.ptratio = lm(crim ~ ptratio, data = Boston)
summary(lm.ptratio)
```

Based on the p-value (2.94e-11), ptratio has a significant association with crim

```{r}
lm.black = lm(crim~black)

```

## Model: crim ~ lstat

$crim = \beta_0 + \beta_1 lstat$

```{r}
lm.lstat = lm(crim ~ lstat, data = Boston)
summary(lm.lstat)
```

Based on the p-value (2e-16), lstat has a significant association with crim

## Model: crim ~ medv

$crim = \beta_0 + \beta_1 medv$

```{r}
lm.medv = lm(crim ~ medv, data = Boston)
summary(lm.medv)
```

Based on the p-value (2e-16), tax has a significant association with crim

## Diagnostic Plots

```{r}
par(mfrow = c(2,2))
plot(lm.indus)
plot(lm.chas)
plot(lm.nox)
plot(lm.rm)
plot(lm.age)
plot(lm.dis)
plot(lm.rad)
plot(lm.tax)
plot(lm.ptratio)
plot(lm.lstat)
plot(lm.medv)
```

# 15b Multiple Linear Regression

(b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis $H_0 : \beta_j = 0$?

***

## Fitting on all independent variables

```{r}
lm.all = lm(crim ~ ., data=Boston)
summary(lm.all)
```

Based on the MLR, only zn, dis, rad, black, and medv have a significant association with crim (p-value is below 0.05) which means we can reject the null hypothesis

# 15c

(c) How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.

***

## Coefficients

```{r}
x = c(coefficients(lm.zn)[2],
      coefficients(lm.indus)[2],
      coefficients(lm.chas)[2],
      coefficients(lm.nox)[2],
      coefficients(lm.rm)[2],
      coefficients(lm.age)[2],
      coefficients(lm.dis)[2],
      coefficients(lm.rad)[2],
      coefficients(lm.tax)[2],
      coefficients(lm.ptratio)[2],
      coefficients(lm.black)[2],
      coefficients(lm.lstat)[2],
      coefficients(lm.medv)[2])
y = coefficients(lm.all)[2:14]
```

## Plot Coefficients
```{r}
par(mfrow = c(1,1)) # 1 plot
plot(x, y)
```

x$coeffients <-  c('Intercept', "RME")


# 15d Non-Linear Association

(d) Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form

$$Y = \beta_0 + \beta_1X + \beta_2 X^2 + \beta_3 X^3 + \epsilon$$

***

## Model: crim ~ zn with 3rd degree polynomials

$$Y = \beta_0 + \beta_1 (zn) + \beta_2 (zn)^2 + \beta_3 (zn)^3 + \epsilon$$
```{r}
lm.poly.zn = lm(crim ~ zn + I(zn^2) + I(zn^3), data = Boston)
summary(lm.poly.zn)
```

Based on the p-values, zn does NOT have a non-linear association with crim

## Diagnostic Plots

```{r}
par(mfrow = c(2,2))
plot(lm.zn)
```

## Model: crim ~ indus with 3rd degree polynomials

$Y = \beta_0 + \beta_1 (indus) + \beta_2 (indus)^2 + \beta_3 (indus)^3 + \epsilon$

```{r}
lm.poly.indus = lm(crim ~ indus + I(indus^2) + I(indus^3), data = Boston)
summary(lm.poly.indus)
```

Based on the p-values, indus SHOWS that it has a non-linear association with crim

## Model: crim ~ chas with 3rd degree polynomials

$Y = \beta_0 + \beta_1 (chas) + \beta_2 (chas)^2 + \beta_3 (chas)^3 + \epsilon$

```{r}
lm.poly.chas = lm(crim ~ chas + I(chas^2) + I(chas^3), data = Boston)
summary(lm.poly.chas)

```

Since chas is a factor, squaring it does not affect it.

## Model: crim ~ nox with 3rd degree polynomials

$Y = \beta_0 + \beta_1 (nox) + \beta_2 (nox)^2 + \beta_3 (nox)^3 + \epsilon$

```{r}
lm.poly.nox = lm(crim ~ nox + I(nox^2) + I(nox^3), data = Boston)
summary(lm.poly.nox)
```

Based on the p-values, nox SHOWS that it has a non-linear association with crim

## Model: crim ~ rm with 3rd degree polynomials

```{r}
lm.poly.rm = lm(crim ~ rm + I(rm^2) + I(rm^3), data = Boston)
summary(lm.poly.rm)
```

Based on the p-value, rm does NOT have a non-linear association with crim

## Model: crim ~ age

```{r}
lm.poly.age = lm(crim ~ age + I(age^2) + I(age^3), data = Boston)
summary(lm.poly.age)
```

Based on the p-values, age SHOWS a non-linear association with crim

## Model: crim ~ dis

```{r}
lm.poly.dis = lm(crim ~ dis + I(dis^2) + I(dis^3), data = Boston)
summary(lm.poly.dis)
```

Based on the p-values, dis SHOWS a non-linear association with crim

## Model: crim ~ rad

## Model Summary

```{r}
lm.poly.rad = lm(crim ~ rad + I(rad^2) + I(rad^3), data = Boston)
summary(lm.poly.rad)
```

Based on the p-value, rad does NOT have a non-linear association with crim

## Model: crim ~ tax with 3rd degree polynomials

```{r}
lm.poly.tax = lm(crim ~ tax + I(tax^2) + I(tax^3), data = Boston)
summary(lm.poly.tax)
```

Based on the p-value, tax does NOT have a non-linear association with crim

## Model: crim ~ ptratio 

```{r}
lm.poly.ptratio = lm(crim ~ ptratio + I(ptratio^2) + I(ptratio^3), data = Boston)
summary(lm.poly.ptratio)
```

Based on the p-value, ptratio SHOWS a non-linear association with crim

## Model: crim ~ lstat

```{r}
lm.poly.lstat = lm(crim ~ lstat + I(lstat^2) + I(lstat^3), data = Boston)
summary(lm.poly.lstat)
```

Based on the p-value, lstat NOT have a non-linear association with crim

## Model: crim ~ medv

```{r}
lm.poly.medv = lm(crim ~ medv + I(medv^2) + I(medv^3), data = Boston)
summary(lm.poly.medv)
```

Based on the p-value, medv SHOWS a non-linear association with crim
